# OpenAI Accuses DeepSeek of Copying its Copycat AI

**Hey, everyone!** Today, weâ€™re diving into the latest drama in the AI worldâ€”OpenAI is mad. Like, really mad. And why? Because DeepSeek, a Chinese AI company, allegedly "distilled" ChatGPTâ€™s knowledge to build its own chatbot. But waitâ€”didnâ€™t OpenAI train ChatGPT on, wellâ€¦ all of the internet? Itâ€™s irony at its finest. Letâ€™s break it down.

---

## **The Accusation**

So, hereâ€™s the deal: OpenAI and Microsoft claim DeepSeek copied them using a technique called **distillation**. Distillation is when a smaller AI model "learns" from a bigger AI model by asking it a ton of questions, analyzing the responses, and essentially **mimicking** how the bigger model thinks. OpenAI alleges that DeepSeek siphoned ChatGPTâ€™s reasoning, allowing it to build a competitive LLM **at a fraction of the cost**.

Now, OpenAI hasnâ€™t provided direct evidence, but they say Microsoftâ€™s security team noticed **suspicious API activity**â€”basically, an unusual number of queries hitting ChatGPT, which may have been DeepSeek gathering data. If true, this might violate OpenAIâ€™s Terms of Service. But letâ€™s be honestâ€”how enforceable is that when OpenAI itself built its empire off massive amounts of freely available (and often copyrighted) data?

---

## **How DeepSeek Actually Works**

Before we pass judgment, letâ€™s talk about what makes DeepSeek-R1 unique. Unlike ChatGPT, which relies heavily on **Supervised Fine-Tuning (SFT)**, DeepSeek-R1 does something differentâ€”it learns **purely through Reinforcement Learning (RL)**. This means itâ€™s not just memorizing dataâ€”itâ€™s figuring things out on its own.

### **Humans Learn in Baby Stepsâ€”So Does DeepSeek**
Humans donâ€™t go from zero to expert overnight. We improve **step by step**, gradually getting better over time. Whether itâ€™s learning to walk, mastering a new language, or picking up a musical instrument, our brains rely on incremental progress. We try, fail, adjust, and try again. 

SFT, focuses on **matching a final goal**â€”it tells the AI exactly what the correct answer is and expects it to learn by mimicking that outcome. This is like handing a student a list of answers without teaching them how to solve the problems.

RL, on the other hand, focuses on **matching steps toward a goal**â€”the AI isnâ€™t given the final answer but is instead rewarded for improving its reasoning along the way. Itâ€™s like teaching a student how to solve a problem by giving them feedback on each step of their work. This allows DeepSeek to develop reasoning skills rather than just memorizing responses.

### **Explaining the Image Using a Video Game Example ğŸ® (with Absolute Reward vs. Baseline Comparison)**

This diagram represents **Reinforcement Learning (RL)**, which is how AI learns to make smart decisions by **trying things, seeing what works, and improving over time**â€”**just like getting better at a video game!**

Think of this as **playing your favorite game** where you control a character and learn how to win. Here's how it works:

---

### ğŸ•¹ **1ï¸âƒ£ The Agent = YOU (the Player)**
- The **Agent** is like **you** when you play a game.
- It **decides what to do** next, just like you press buttons on your controller.

---

### ğŸŒ **2ï¸âƒ£ The Environment = The Game World**
- The **Environment** is the **game world**â€”it responds to what you do.
- If you're playing a racing game, it's the track, the cars, and everything around you.
- If you're playing a shooting game, it's the enemies, weapons, and obstacles.

---

### ğŸ¯ **3ï¸âƒ£ The State = The Current Game Situation**
- The **State (\(S_t\))** is what the game looks like **right now**.
- Example: If you're playing a Mario game, the state tells you **where Mario is, what enemies are near, and how much time is left.**

---

### ğŸ† **4ï¸âƒ£ The Reward = Your Score (Absolute Reward vs. Baseline Comparison)**

In games, rewards can be calculated in **two different ways**:

#### âœ… **(A) Absolute Reward (Direct Score)**
- This is the actual points you earn.
- Example: In a racing game, finishing a lap in **50 seconds gives you 500 points**, and finishing in **40 seconds gives you 600 points**.
- The AI just **sees the raw score** and tries to get the highest possible points.

#### âš– **(B) Baseline Comparison (Relative Score)**
- Instead of looking at just the raw score, we compare it **to an expected or average score** (baseline).
- Example: If most players finish the race in **50 seconds**, the AI compares itself to that **baseline**.
  - If it finishes in **45 seconds**, it gets a **positive reward** (it's doing better than average).
  - If it finishes in **55 seconds**, it gets a **negative reward** (it's doing worse than average).

ğŸ® **Game Example for Baseline Comparison:**
- If you're playing a shooting game and you usually hit **70% of your shots**, then:
  - **Hitting 80%** in a match gives you a **positive reward** (you're doing better).
  - **Hitting 60%** gives you a **negative reward** (you're performing worse).

ğŸ‘€ **Why is this useful?**
- Absolute rewards work when you just need to **maximize points** (like a high score game).
- Baseline comparison helps when you need to **improve over time**, rather than just getting a raw score.

---

### ğŸ® **5ï¸âƒ£ The Action = What You Do Next**
- The **Action (\(A_t\))** is what **you decide to do**.
- Example: In a shooting game, you might decide to **aim and fire**. In a racing game, you might decide to **speed up or drift**.

---

### ğŸ”„ **How It All Loops Together**
1. **You see the game state** (\(S_t\)) â†’ Example: You're low on health, and an enemy is nearby.
2. **You take an action** (\(A_t\)) â†’ You decide to dodge and fire your weapon.
3. **The game responds** (Environment) â†’ You either hit the enemy (**absolute reward: +10 points**, or **baseline: better than your usual accuracy**).
4. **The game updates your situation** (\(S_{t+1}\)) â†’ You move to a new state and repeat.

This cycle keeps repeating, just like you keep **learning and improving at the game** over time.

---

### ğŸš€ **How AI Uses This to Get Good**
- AI **tries different actions** (like you trying new strategies in a game).
- It **learns from rewards**:
  - **Absolute Rewards** â†’ Just get more points!
  - **Baseline Comparison** â†’ Get better than before!
- Over time, AI **figures out the best way to win**â€”just like **you get better at a game the more you play!**

So, **AI in Reinforcement Learning is like a beginner gamer who keeps practicing until they become a pro!** ğŸ®ğŸ”¥


### **Chain of Thought: Thinking Like a Human**
DeepSeek uses a technique called **Chain of Thought (CoT) prompting**, where it breaks down problems step by step, just like you would when solving a math problem. But CoT doesnâ€™t just ask for the final answerâ€”it demands that the model **show its work**. By explicitly outlining its reasoning process, DeepSeek can make its thought process transparent. 

This is crucial because if the model makes a mistake, it can **self-correct** and re-evaluate where it went wrong. Itâ€™s the AI equivalent of a student realizing they miscalculated a step in a math problem and going back to fix it. This iterative reasoning is one of the reasons why DeepSeek performs so well on complex tasks like mathematics and coding.

### **Reinforcement Learning: The Baby Learning to Walk**
Instead of being spoon-fed correct answers, DeepSeek learns by trial and errorâ€”kind of like how a baby learns to walk. It stumbles, makes mistakes, and figures out what works best. This is why its reasoning gets sharper over time.

### **Group Relative Policy Optimization: Smarter Learning**
DeepSeek refines its learning through an algorithm called **Group Relative Policy Optimization (GRPO)**, which is just a fancy way of saying it improves itself **without needing an enormous separate AI to judge its answers**. That makes its training process **way more efficient** than OpenAIâ€™s methods.
DeepSeek refines its learning through an algorithm called **Group Relative Policy Optimization (GRPO)**, which is just a fancy way of saying it improves itself without needing an enormous separate AI to judge its answers. That makes its training process way more efficient than OpenAIâ€™s methods.

## GRPO Equation

```math
GRPO(\theta) = \mathbb{E}_{q \in Q, o_i \in \Pi_{old}} \left[ \frac{1}{G} \sum_{i=1}^{G} \min \left( \frac{\Pi_{\theta}(o_i | q)}{\Pi_{old}(o_i | q)}, 1 + \epsilon \right) A_i \right] - \beta D_{KL} (\Pi_{\theta} || \Pi_{ref})
```

Now, let's break this down into **5 simple parts**, explaining it like you're a teenager who loves video games:

---

## 1ï¸âƒ£ The Expectation Term

```math
\mathbb{E}_{q \in Q, o_i \in \Pi_{old}}
```

ğŸ” **What it means:**  
This just means **"on average"** over all the data we have.  

ğŸ® **Game Example:**  
Imagine youâ€™re playing a game and trying to find the best way to win. You look at all the strategies you've used before and see which ones worked best. This expectation part is like averaging how well different strategies performed.

---

## 2ï¸âƒ£ The Policy Likelihood Ratio

```math
\frac{\Pi_{\theta}(o_i | q)}{\Pi_{old}(o_i | q)}
```

ğŸ” **What it means:**  
This fraction compares **the new way of doing things** (\(\Pi_{\theta}\)) with **the old way** (\(\Pi_{old}\)). It shows how different the new strategy is from the old one.

ğŸ® **Game Example:**  
Letâ€™s say you used to aim for the head in a shooting game, but now youâ€™re trying to aim for the chest. This ratio tells you **how much your new aiming style has changed compared to the old one**.

---

## 3ï¸âƒ£ The Clipping Mechanism

```math
\min \left( \frac{\Pi_{\theta}(o_i | q)}{\Pi_{old}(o_i | q)}, 1 + \epsilon \right) A_i
```

ğŸ” **What it means:**  
This prevents the AI from making **huge, reckless changes** to its strategy by limiting how much it can update at a time.

ğŸ® **Game Example:**  
Imagine if every time you lost a round, you completely changed your playstyleâ€”switching from aggressive to ultra-defensive. That would be chaotic! Instead, this clipping makes sure you **only make small, smart adjustments** so you donâ€™t mess up completely.

---

## 4ï¸âƒ£ The Advantage Term

```math
A_i = R_i - R_{mean}
```

ğŸ” **What it means:**  
This part measures how much **better or worse** an action is compared to the average.

ğŸ® **Game Example:**  
If your usual game score is **100 points**, but you just scored **150 points**, then you know this new strategy is working great! If you only scored **80 points**, then you should rethink your approach. This part helps the AI **learn from what works best**.

---

## 5ï¸âƒ£ KL Divergence Term

```math
- \beta D_{KL} (\Pi_{\theta} || \Pi_{ref})
```

ğŸ” **What it means:**  
This makes sure the AI doesnâ€™t change **too much** and stays similar to what has worked before.

ğŸ® **Game Example:**  
Imagine you get really good at one game strategy, but suddenly try something totally random that makes no sense. This term **pulls you back** so you donâ€™t drift too far away from what actually works.

---

## Summary

GRPO is like training to be a **better gamer**:
- **Step 1:** Look at what worked before.  
- **Step 2:** Compare your new strategy to the old one.  
- **Step 3:** Make small, controlled improvements.  
- **Step 4:** Learn from actions that really help you win.  
- **Step 5:** Don't change too much at onceâ€”stick to what works!  

By following these steps, **DeepSeek trains itself in a smart way without needing a big separate AI to judge it**â€”making it **faster and more efficient** than OpenAIâ€™s older training methods! ğŸš€


By following these steps, **DeepSeek trains itself in a smart way without needing a big separate AI to judge it**â€”making it **faster and more efficient** than OpenAIâ€™s older training methods! ğŸš€

### **Distillation: The Core of OpenAIâ€™s Complaint**
And finally, the controversial part: **distillation**. DeepSeek takes what it learned and distills it into **smaller, more efficient models**â€”ones that can run on far less computing power. In fact, DeepSeekâ€™s **14B model beats OpenAIâ€™s o1-mini**, showing that distillation isnâ€™t just copyingâ€”itâ€™s optimizing. Itâ€™s taking knowledge and making it more efficient, accessible, and affordable.

But, of course, OpenAI sees it differently. They claim DeepSeek â€œstoleâ€ from them, despite the fact that distillation is a well-established AI practice. The irony? OpenAI itself has **used distillation techniques** to improve its own models.

---

## **Ethical Dilemma: Who Owns AI Knowledge?**

This whole situation raises a huge ethical question: **Who actually owns AI-generated knowledge?**

- OpenAI built ChatGPT by training it on **publicly available data**â€”a lot of which was copyrighted.
- DeepSeek allegedly trained its AI by using OpenAIâ€™s AI outputs.
- But if OpenAI didnâ€™t create its knowledge from scratch, **can it really claim ownership over it?**

At the end of the day, AI models are **only as good as their training data**â€”and if OpenAIâ€™s argument is that DeepSeek copied from them, then by extension, DeepSeek copied from data that **wasnâ€™t even OpenAIâ€™s to begin with**.

---

## **Whatâ€™s Next? AI Wars Begin**

OpenAI has made it clear theyâ€™re not going to let this slide. Insiders say theyâ€™re working on methods to **prevent distillation**, which could mean tightening API restrictions or adding noise to outputs to make copying harder. But realistically? **The AI race is global now.**

And guess what? As of right now, **DeepSeek is the #1 AI app on the Apple App Store.** Thatâ€™s rightâ€”despite OpenAIâ€™s complaints, users are voting with their downloads.

So, what do you think? Is this just another case of AI companies playing dirty while pretending they own intelligence itself? Letâ€™s talk about it in the comments!

**Donâ€™t forget to like, subscribe, and hit that bell. Thanks for watching the Bear Brown and Company YouTube Channel.**




