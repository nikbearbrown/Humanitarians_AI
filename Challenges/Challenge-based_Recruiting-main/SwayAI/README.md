# Sway AI Programming Challenge for Internship / Job Applicants

## Objective:
Develop a comprehensive data cleaning solution that can identify, classify, and correct data errors, learn from the previous cleaning sessions, and be integrable and scalable with the existing data cleaning tools and processes.

## Target Participants:
Master’s degree students or recent graduates in Computer Science, Data Science, Machine Learning, or related fields.

## Time Allocation:
A maximum of one weekend (48 hours) from the initiation of the challenge.

## General Requirements:
1. **Error Identification and Classification**
   - Identify various types of data errors (e.g., missing, outlier, duplicate, inconsistent data).
   - Classify the identified errors based on predefined or dynamically determined categories.

2. **Error Correction**
   - Implement mechanisms to correct identified data errors automatically, ensuring accuracy and minimal data distortion.

3. **Continuous Learning**
   - Develop a model that can learn and adapt from previous data cleaning sessions, enhancing its ability to identify and rectify errors efficiently over time.

4. **Integration Capabilities**
   - Ensure compatibility and ease of integration with existing data cleaning tools and processes.

5. **Scalability**
   - Demonstrate that the solution can effectively handle and process large volumes of data without compromising performance or accuracy.

6. **Documentation**
   - Provide a comprehensive documentation outlining design choices, architecture, usage instructions, and any potential improvements that could be pursued in the future.

## Challenge Details:
Applicants should choose one of the following challenges. Questions should be sent to:  
- [sumit.kumar@sway-ai.com](mailto:sumit.kumar@sway-ai.com)
- [jitender.arora@sway-ai.com](mailto:jitender.arora@sway-ai.com)


### Challenge One: Data Input
**Data Ingestion:**
- Implement functionality to ingest data from various sources, which may include APIs (e.g., Google), cloud data platforms (e.g., Snowflake, Databricks), and file formats (e.g., CSV).
- Ensure support for structured and semi-structured data types.
- Implement a mechanism to validate the ingested data in terms of format and consistency.

**Data Security and Compliance:**
- Ensure that the data ingestion process adheres to data security norms and privacy regulations (like GDPR or CCPA). This includes secure data transmission, storage, and access controls.
- Implement mechanisms to anonymize or pseudonymize sensitive data, if applicable, ensuring compliance without compromising data utility.

### Challenge Two: Data Preprocessing
**Data Normalization and Standardization:**
- Implement techniques to normalize and standardize the data, ensuring consistency across different data points and features.

**Handling Missing Values:**
- Develop strategies for managing missing data through imputation or deletion while maintaining data integrity and usefulness.

**Outlier Detection and Handling:**
- Create mechanisms for detecting and appropriately dealing with outliers in the dataset.

**Feature Selection and Transformation:**
- Implement methods for robust feature selection and transformation to enhance the effectiveness of the cleaning process.

### Challenge Three: Data Quality Assessment
**Automatic Identification of Issues:**
- Implement automation for identifying data quality issues like duplicates, inconsistencies, and errors, ensuring a consistent and reliable dataset.

**Data Quality Thresholds:**
- Establish rules and thresholds for assessing data quality and automatically flagging or addressing issues.

### Challenge Four: Customization [Priority 2 (P2)]
**Configurable Cleaning Rules:**
- Develop the capability to configure data cleaning rules to accommodate domain-specific requirements.

**Parameter Tuning:**
- Allow flexibility in parameter tuning and model selection to optimize performance across various use cases.

### Challenge Five: Scalability
**Handling Large Data Volumes:**
- Ensure that the solution can handle large volumes of data efficiently without compromising performance or data quality.

**Scalable Architecture:**
- Design the architecture to be scalable, accommodating growing datasets and evolving requirements.

### Challenge Six: Monitoring and Reporting
**Real-time Monitoring:**
- Implement real-time monitoring of data cleaning processes to ensure ongoing accuracy and reliability.

**Logging and Alerting:**
- Develop logging and alerting mechanisms for errors or anomalies encountered during data processing.

**Quality Improvement Reporting:**
- Generate reports highlighting data quality improvements and ongoing issues.

**Version Control: [To Be Determined (TBD)]**
- Implement version control for data processing elements.

### Challenge Seven: Data Versioning and Audit Trails [P2]
**Record Keeping:**
- Maintain a comprehensive record of data changes to ensure traceability and compliance with regulatory requirements.

**Audit Trails:**
- Support data lineage and audit trails to facilitate comprehensive data audits and assessments.

### Challenge Eight: Update Existing User Interface
**User-friendly Visualizations:**
- Develop user-friendly visualizations for monitoring and configuring the data cleaning process.

**User Support:**
- Ensure that the interface is supportive and accessible for both technical and non-technical users.

### Challenge Nine: Error Handling
**Graceful Error Handling:**
- Implement graceful error handling and recovery mechanisms to ensure minimal disruption during data cleaning failures.

**Notification Mechanisms:**
- Develop notification mechanisms to alert relevant stakeholders in the event of data cleaning failures.

### Challenge Ten: Cost Analysis
**Cost Evaluation:**
- Evaluate the overall cost of data cleaning processes, identifying potential areas for optimization and improvement.

**Optimization:**
- Implement strategies and mechanisms to optimize the cost-efficiency of data cleaning processes.

## Note to Participants:
Ensure that each solution developed for the challenges is modular, scalable, and effectively documented. Prioritize challenges based on the information provided and manage your time effectively to create viable solutions within the allocated timeframe. Remember, quality, innovation, and attention to detail are crucial in your submissions. Good luck!

## Evaluation Criteria:
- **Functionality:** Does the solution meet the described requirements and solve the proposed problems effectively?
- **Usability:** Is the solution user-friendly, and is the documentation clear and comprehensive?
- **Efficiency:** How optimally does the solution utilize resources, and how well does it scale with increasing data volumes?
- **Adaptability:** How effectively does the solution learn from past cleaning sessions and improve subsequent operations?
- **Innovation:** Is there a novel approach or technology implemented that enhances the solution’s effectiveness or efficiency?
- **Code Quality:** How clean, organized, and maintainable is the code?
- **Security & Compliance:** How effectively does the solution adhere to data security and privacy compliance norms?

## Submission:
Provide a GitHub repository link containing all code, documentation, and relevant assets.
Include a README file with clear instructions on setting up and running the solution, along with a brief summary of the approach taken.

For inquiries, submissions, or any questions, please feel free to contact us using the following email addresses:

- [sumit.kumar@sway-ai.com](mailto:sumit.kumar@sway-ai.com)
- [jitender.arora@sway-ai.com](mailto:jitender.arora@sway-ai.com)

We welcome your communication and look forward to assisting you.


## Note:
Please ensure to thoroughly test the solution to ensure reliability and efficacy. Remember to manage your time effectively to cover all requirements within the allotted time frame.

Good luck, and we look forward to reviewing your innovative solutions to this challenge!
